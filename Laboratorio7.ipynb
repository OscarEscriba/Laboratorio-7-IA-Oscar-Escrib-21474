{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Teoría\n",
    "\n",
    "### 1. ¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje supervisado?\n",
    "El **Temporal Difference Learning (TD Learning)** es un método de aprendizaje por refuerzo que combina ideas del aprendizaje supervisado y el aprendizaje dinámico. Se diferencia de los métodos tradicionales de aprendizaje supervisado en que no requiere un conjunto de datos etiquetado de antemano, sino que aprende a partir de la experiencia directa con un entorno, ajustando sus estimaciones de valor en cada paso basándose en la diferencia entre las predicciones sucesivas.\n",
    "\n",
    "#### Error de diferencia temporal:\n",
    "El **error de diferencia temporal** mide la discrepancia entre la estimación actual del valor de un estado y la estimación corregida basada en la recompensa obtenida y la estimación del siguiente estado:\n",
    "\n",
    "\\[ TD\\_error = R_t + \\gamma V(S_{t+1}) - V(S_t) \\]\n",
    "\n",
    "Donde:\n",
    "- \\( R_t \\) es la recompensa obtenida en el tiempo \\( t \\)\n",
    "- \\( \\gamma \\) es el factor de descuento\n",
    "- \\( V(S_t) \\) es el valor estimado del estado actual\n",
    "- \\( V(S_{t+1}) \\) es el valor estimado del siguiente estado\n",
    "\n",
    "Este error se usa para actualizar la estimación del valor de los estados, permitiendo el aprendizaje sin necesidad de esperar hasta el final de un episodio.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Juegos Simultáneos y Toma de Decisiones\n",
    "En los **juegos simultáneos**, los jugadores toman decisiones sin conocer previamente las acciones de sus oponentes. Esto implica que deben formular estrategias basadas en la suposición de lo que harán los demás jugadores.\n",
    "\n",
    "#### Ejemplo del mundo real:\n",
    "Un ejemplo de un **juego simultáneo** es el mercado de precios entre dos empresas competidoras. Si dos compañías venden productos similares, deben decidir simultáneamente el precio sin conocer la decisión de su competidor. Las estrategias pueden incluir:\n",
    "- **Estrategia conservadora:** Mantener precios estables.\n",
    "- **Estrategia agresiva:** Reducir precios para captar más mercado.\n",
    "- **Estrategia mixta:** Ajustar precios de manera probabilística basada en el comportamiento pasado del oponente.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Juegos de Suma Cero vs. No Suma Cero\n",
    "Los **juegos de suma cero** son aquellos donde la ganancia de un jugador es exactamente igual a la pérdida del otro. En cambio, en los **juegos de no suma cero**, los jugadores pueden obtener beneficios mutuos o compartir pérdidas.\n",
    "\n",
    "#### Ejemplo de juego de no suma cero:\n",
    "Un caso clásico es la **negociación salarial** entre un empleador y un trabajador. Si ambas partes cooperan, pueden encontrar un acuerdo en el que ambos se beneficien (por ejemplo, un salario justo con beneficios). Las estrategias incluyen:\n",
    "- **Cooperación:** Buscar un acuerdo justo para ambas partes.\n",
    "- **Competencia:** Intentar maximizar la propia ganancia a expensas del otro.\n",
    "- **Compromiso:** Ceder en ciertos aspectos para obtener mejores términos en otros.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Equilibrio de Nash en Juegos Simultáneos\n",
    "El **equilibrio de Nash** es un concepto en el cual, dado que todos los jugadores han elegido sus estrategias, ningún jugador tiene incentivos para cambiar unilateralmente su decisión, ya que esto no mejoraría su resultado.\n",
    "\n",
    "#### Ejemplo:\n",
    "En el **dilema del prisionero**, si ambos jugadores eligen la estrategia de \"confesar\", están en un equilibrio de Nash porque ninguno mejora su situación cambiando su decisión unilateralmente.\n",
    "\n",
    "En juegos simultáneos, el equilibrio de Nash ayuda a identificar estrategias óptimas en situaciones de incertidumbre, permitiendo a los jugadores anticipar las acciones de sus oponentes.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Aplicación del Temporal Difference Learning en Procesos de Toma de Decisiones\n",
    "El **TD Learning** se usa en múltiples aplicaciones donde los sistemas deben aprender en entornos dinámicos, como en el control de robots, juegos de mesa (ejemplo: AlphaGo), y en optimización de procesos industriales.\n",
    "\n",
    "#### Exploración vs. Explotación\n",
    "El algoritmo maneja este equilibrio mediante:\n",
    "- **Exploración (Exploration):** Intentar nuevas acciones para descubrir mejores estrategias.\n",
    "- **Explotación (Exploitation):** Usar la mejor estrategia conocida hasta el momento.\n",
    "\n",
    "Una técnica común para manejar este balance es el **ε-greedy**, donde se elige la mejor acción con probabilidad \\( 1 - \\epsilon \\) y una acción aleatoria con probabilidad \\( \\epsilon \\).\n",
    "\n",
    "#### Desafíos en la implementación:\n",
    "- **Convergencia lenta**: En entornos complejos, la convergencia puede requerir muchas iteraciones.\n",
    "- **Riesgo de sobreajuste**: Si se optimiza demasiado para un entorno específico, puede no generalizar bien a otros contextos.\n",
    "- **Exploración insuficiente**: Si \\( \\epsilon \\) es demasiado bajo, el modelo puede quedar atrapado en una estrategia subóptima.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
